{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge : prediction of the sex of individuals based on their picture.\n",
    "###  by Benjamin LAZARD\n",
    "\n",
    "First, let us import all libraries that we will use for this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#basic python packages for plotting and array management\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for data import\n",
    "import pandas as pd\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, RidgeClassifierCV, PassiveAggressiveClassifier, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Ensemble methods and crossvalidation\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier,RandomForestClassifier, VotingClassifier\n",
    "\n",
    "#PostProcessing\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "#Because oh boy some computations take an amazing amount of time !\n",
    "import time\n",
    "\n",
    "# To compute the personalized score\n",
    "def compute_pred_score(y_true, y_pred):\n",
    "    y_pred_unq =  np.unique(y_pred)\n",
    "    for i in y_pred_unq:\n",
    "        if((i != -1) & (i!= 1) & (i!= 0) ):\n",
    "            raise ValueError('The predictions can contain only -1, 1, or 0!')\n",
    "    y_comp = y_true * y_pred\n",
    "    score = float(10*np.sum(y_comp == -1) + np.sum(y_comp == 0))\n",
    "    score /= y_comp.shape[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let us import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_fname = 'training_templates.csv'\n",
    "y_train_fname = 'training_labels.txt'\n",
    "X_test_fname  = 'testing_templates.csv'\n",
    "X_train = pd.read_csv(X_train_fname, sep=',', header=None).values\n",
    "X_test  = pd.read_csv(X_test_fname,  sep=',', header=None).values\n",
    "y_train = np.loadtxt(y_train_fname, dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us sum up what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will train our algorithm based on a set of 105600 pictures, each with 128 features.\n",
      "Then we will test it on a set of 8496 pictures with the same number of features.\n",
      "\n",
      "The training set consists of labels: \n",
      "[-1  1]\n",
      "for exemple '-1' = women and '1' = men\n",
      "There are exactly 52800 men and 52800 women\n"
     ]
    }
   ],
   "source": [
    "print(\"We will train our algorithm based on a set of %d pictures, each with %d features.\"%(X_train.shape[0],X_train.shape[1]))\n",
    "print(\"Then we will test it on a set of %d pictures with the same number of features.\"%(X_test.shape[0]))\n",
    "print(\"\\nThe training set consists of labels: \")\n",
    "print(np.unique(y_train))\n",
    "print(\"for exemple '-1' = women and '1' = men\")\n",
    "print(\"There are exactly %d men and %d women\" %((y_train == -1).sum(),(y_train == 1).sum() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "trying to preprocess the data, and check the difference in score for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression / training test / regular score= 0.639\n",
      "Logistic regression / training test / scaled score= 0.636\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = preprocessing.scale(X)\n",
    "\n",
    "#score of logistic regression with regular data\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Logistic regression / training test / regular score= %0.3f\"%(score))\n",
    "\n",
    "#Score with standardized data\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred_train = clf.predict(X_train_scaled)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Logistic regression / training test / scaled score= %0.3f\"%(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at least for this particular example, scaling does not seem very useful\n",
    "\n",
    "# Step 2\n",
    "## RidgeClassifier\n",
    "Trying a few linear models on both the regular and scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier / training test / regular / score= 0.646\n",
      "RidgeClassifier / training test / normalized / score= 0.867\n"
     ]
    }
   ],
   "source": [
    "#Classification based on ridge regression\n",
    "clf = RidgeClassifier(normalize=False)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"RidgeClassifier / training test / regular / score= %0.3f\"%(score))\n",
    "\n",
    "#same thing, but with normalzed data\n",
    "clf = RidgeClassifier(normalize=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"RidgeClassifier / training test / normalized / score= %0.3f\"%(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can deduce that\n",
    "- It does take long to process\n",
    "- Normalized data still do not score very well\n",
    "\n",
    "Because this method is very fast to compute, we can now try to optimize the regularization paramter alpha performing a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifierCV / training test / regular / score= 0.646\n",
      "chosen alpha =0.10\n"
     ]
    }
   ],
   "source": [
    "clf = RidgeClassifierCV(alphas = [1e-7, 1e-5, 1e-3, 1e-2, 0.1 , 1, 10, 1e2, 1e3, 1e5, 1e7], normalize=False)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"RidgeClassifierCV / training test / regular / score= %0.3f\"%(score))\n",
    "print(\"chosen alpha = %.2f\"%(clf.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can narrow our search around 0.1, after several tries, I was led to the following test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifierCV / training test / regular / score= 0.646\n",
      "chosen alpha =0.350\n"
     ]
    }
   ],
   "source": [
    "clf = RidgeClassifierCV(alphas = [0.32, 0.33, 0.34, 0.345, 0.35, 0.355, 0.36], normalize=False)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"RidgeClassifierCV / training test / regular / score= %0.3f\"%(score))\n",
    "print(\"chosen alpha =%.3f\"%(clf.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing that the score cannot be better that 0.646 with $\\alpha = 0.35$\n",
    "So let us move on to another classifier such as... \n",
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron / training test / regular / score= 0.902\n",
      "Perceptron / training test / regular / score= 0.921 for params={'eta0': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "#Simple perceptron\n",
    "clf = Perceptron()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Perceptron / training test / regular / score= %0.3f\"%(score))\n",
    "\n",
    "#Fine tuning its outcome (after several tries for eta0)\n",
    "tuned_parameters = [{'eta0': [0.0001, 0.005, 0.01, 0.02, 1],}]\n",
    "clf = GridSearchCV(Perceptron(n_jobs=-1, n_iter=10), param_grid= tuned_parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Perceptron / training test / regular / score= %0.3f for params= %a\"%(score,clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still no better than the logisitic regressor. And the eta0 parameter seems to have only very little impact. Scaled data gave even worse results and therefore are not shown here.\n",
    "Let us move on to a variation of the Perceptron :\n",
    "## the Passive AgressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassiveAggressiveClassifier / training test / regular / score= 0.765\n",
      "PassiveAggressiveClassifier / training test / regular / score= 0.683 for params= {'n_jobs': -1, 'loss': 'hinge', 'n_iter': 20, 'C': 0.04}\n"
     ]
    }
   ],
   "source": [
    "#Simple PassiveAggressiveClassifier\n",
    "clf = PassiveAggressiveClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"PassiveAggressiveClassifier / training test / regular / score= %0.3f\"%(score))\n",
    "\n",
    "#Fine tuning its outcome (after several tries)\n",
    "tuned_parameters = [{'C': [ 0.01, 0.02, 0.025, 0.03, 0.04, 0.1],\n",
    "                     'loss': ['hinge', 'squared_hinge'],\n",
    "                     'n_iter': [10, 20],\n",
    "                     'n_jobs' :[-1]\n",
    "                    }]\n",
    "clf = GridSearchCV(PassiveAggressiveClassifier(n_iter=10, n_jobs=2), param_grid= tuned_parameters, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"PassiveAggressiveClassifier / training test / regular / score= %0.3f for params= %a\"%(score,clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is still worse than the **logisticRegressor**, but it is not so bad.\n",
    "## Let us try out the SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier / training test / regular / score= 0.648\n",
      "SGDClassifier / training test / regular / score= 0.630 for params= {'penalty': 'l2', 'eta0': 0.1, 'n_iter': 10, 'learning_rate': 'invscaling', 'n_jobs': -1, 'alpha': 0.0001, 'loss': 'hinge'}\n"
     ]
    }
   ],
   "source": [
    "#Simple SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"SGDClassifier / training test / regular / score= %0.3f\"%(score))\n",
    "\n",
    "#Fine tuning its outcome (after several tries)\n",
    "sgd_tuned_parameters = [{'loss': ['log', 'hinge'],\n",
    "                         'penalty' : ['none', 'l2'],\n",
    "                         'alpha': [0.0001],\n",
    "                         'eta0' : [0.008, 0.1, 0.2],\n",
    "                         'learning_rate': ['optimal'],\n",
    "                         'n_iter': [10],\n",
    "                         'n_jobs' :[-1],\n",
    "                        },\n",
    "                        {'loss': ['log', 'hinge'],\n",
    "                         'penalty' : ['l2', 'elasticnet'],\n",
    "                         'alpha': [0.0001],\n",
    "                         'learning_rate': ['constant', 'invscaling'],\n",
    "                         'eta0' : [0.001, 0.1, 1, 10],\n",
    "                         'n_iter': [10],\n",
    "                         'n_jobs' :[-1],\n",
    "                        },\n",
    "                        {'loss': ['log', 'hinge'],\n",
    "                         'penalty' : ['l2'],\n",
    "                         'alpha': [0.0001],\n",
    "                         'learning_rate': ['invscaling'],\n",
    "                         'eta0' : [0.01, 0.1, 0.5, 1],\n",
    "                         'power_t' :[0.1, 0.5, 1],\n",
    "                         'n_iter': [10],\n",
    "                         'n_jobs' :[-1],\n",
    "                        },\n",
    "                        {'loss': ['hinge'],\n",
    "                         'penalty' : ['elasticnet'],\n",
    "                         'l1_ratio' : [0.05, 0.1, 0.15],\n",
    "                         'alpha': [0.0001],\n",
    "                         'learning_rate': ['invscaling'],\n",
    "                         'eta0' : [0.008, 0.1, 0.2],\n",
    "                         'power_t' :[0.2, 0.5, 0.7],\n",
    "                         'n_iter': [10],\n",
    "                         'n_jobs' :[-1],\n",
    "                        }]\n",
    "clf = GridSearchCV(SGDClassifier(), param_grid= sgd_tuned_parameters, n_jobs= -1, cv=5)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "y_pred_train = clf.predict(X_train_scaled)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"SGDClassifier / training test / regular / score= %0.3f for params= %a\"%(score, clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested, and it works a lot better with scaled data\n",
    "The true grid is \n",
    "```python\n",
    "    sgd_tuned_parameters = [{'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "                     'penalty' : ['none', 'l2', 'l1', 'elasticnet'],\n",
    "                     'alpha': [0.0001],\n",
    "                     'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "                     'eta0' : [0.0001, 0.001, 0.1, 1],\n",
    "                     'n_iter': [20],\n",
    "                     'n_jobs' :[-1],\n",
    "                     'power_t' :[0.5]\n",
    "                    }]\n",
    "```\n",
    "But is takes too much time to run as it involves a lot of parameters. Moreover, it is irrelevant to test power_t when there is no invscaling involved for example.\n",
    "\n",
    "**score= 0.636 for params= {'learning_rate': 'optimal', 'loss': 'log', 'n_iter': 20, 'penalty': 'none', 'n_jobs': -1, 'alpha': 0.0001}**  \n",
    "led me to choose alpha, keep the log and hinge loss only in mind  \n",
    "**score= 0.637 for params= {'eta0': 1, 'loss': 'log', 'n_iter': 10, 'penalty': 'elasticnet', 'learning_rate': 'invscaling', 'n_jobs': -1, 'alpha': 0.0001}**\n",
    "\n",
    "OK, so I get a better score using  \n",
    "**score= 0.632 for params= {'penalty': 'elasticnet', 'eta0': 0.1, 'n_iter': 10, 'l1_ratio': 0.1, 'learning_rate': 'invscaling', 'n_jobs': -1, 'power_t': 0.5, 'alpha': 0.0001, 'loss': 'hinge'}**\n",
    "\n",
    "For the first time, we get something better than with logisticregressions  \n",
    "**score= 0.630 for params= {'penalty': 'l2', 'eta0': 0.1, 'n_iter': 10, 'learning_rate': 'invscaling', 'n_jobs': -1, 'alpha': 0.0001, 'loss': 'hinge'}**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "Now that we have tried all the linear models, as computing time drastically increases with other methods, we will have to adapt the data for other tries (maybe then, once we find good estimators, we can train them on the full set of features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 features selected out of 128 (70 %) for LDA\n",
      "(105600, 1)\n",
      "LDAClassifier / training test / regular / score= 0.647 for dimensionality= 90\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction ratio:\n",
    "ratio_dr = 70 # as a percentage\n",
    "my_ncomp = round(X_train.shape[1]*ratio_dr/100) # effective number of features retained\n",
    "print(\"%d features selected out of %d (%d %%) for LDA\"%(my_ncomp, X_train.shape[1], ratio_dr))\n",
    "\n",
    "#Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=my_ncomp, solver='svd').fit(X_train, y_train)\n",
    "X_lda = lda.transform(X_train)\n",
    "print(X_lda.shape)\n",
    "\n",
    "y_pred_train = lda.predict(X_train)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"LDAClassifier / training test / regular / score= %0.3f for dimensionality= %a\"%(score, my_ncomp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 features selected out of 128 (100 %) for PCA\n",
      "[ 31.  23.  22.  21.  21.  20.  20.  20.  19.  18.  18.  18.  17.  17.  16.\n",
      "  16.  16.  15.  15.  15.  14.  14.  14.  13.  13.  12.  12.  12.  12.  12.\n",
      "  11.  11.  11.  11.  11.  10.  10.  10.  10.  10.   9.   9.   9.   9.   9.\n",
      "   9.   8.   8.   8.   8.   8.   8.   8.   8.   7.   7.   7.   7.   7.   7.\n",
      "   7.   7.   6.   6.   6.   6.   6.   6.   6.   6.   6.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   4.   4.   4.   4.   4.   4.   4.   4.   4.\n",
      "   4.   4.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.\n",
      "   2.   2.   2.   2.   2.   2.   2.   2.   2.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.   0.   0.]\n",
      "SGD / training test / pca nf= 128 / score= 0.629\n",
      "[ 31.  23.  22.  21.  21.  20.  20.  20.  19.  18.  18.  18.  17.  17.  16.\n",
      "  16.  16.  15.  15.  15.  14.  14.  14.  13.  13.  12.  12.  12.  12.  12.\n",
      "  11.  11.  11.  11.  11.  10.  10.  10.  10.  10.   9.   9.   9.   9.   9.\n",
      "   9.   8.   8.   8.   8.   8.   8.   8.   8.   7.   7.   7.   7.   7.   7.\n",
      "   7.   7.   6.   6.   6.   6.   6.   6.   6.   6.   6.   5.   5.   5.   5.\n",
      "   5.   5.]\n",
      "SGD / training test / pca nf= 77 / score= 1.033\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction ratio:\n",
    "ratio_dr = 100 # as a percentage\n",
    "my_ncomp = round(X_train.shape[1]*ratio_dr/100) # effective number of features retained\n",
    "print(\"%d features selected out of %d (%d %%) for PCA\"%(my_ncomp, X_train.shape[1], ratio_dr))\n",
    "\n",
    "#Principal components analysis\n",
    "pca = PCA(n_components=my_ncomp, svd_solver='full', whiten=True).fit(X_train)\n",
    "X_pca = pca.transform(X_train)\n",
    "\n",
    "#Explained variance ratio\n",
    "print((pca.explained_variance_ratio_*1000).round())\n",
    "\n",
    "#Classify with the best classifier found earlier\n",
    "clf = SGDClassifier(penalty='l2', eta0=0.1, n_iter=10, learning_rate='invscaling', n_jobs=-1, alpha=0.0001, loss='hinge')#penalty='l2', eta0=0.1, n_iter=10, learning_rate='invscaling', n_jobs=-1, alpha=0.0001, loss='hinge'\n",
    "clf.fit(X_pca, y_train)\n",
    "y_pred_train = clf.predict(X_pca)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"SGD / training test / pca nf= %d / score= %0.3f\"%(my_ncomp, score))\n",
    "\n",
    "#Because of the previous analysis, we can try to simplify the number of features:\n",
    "ratio_dr = 60\n",
    "my_ncomp = round(X_train.shape[1]*ratio_dr/100)\n",
    "pca = PCA(n_components=my_ncomp, svd_solver='full', whiten=True).fit(X_train)\n",
    "X_pca = pca.transform(X_train)\n",
    "print((pca.explained_variance_ratio_*1000).round())\n",
    "clf = SGDClassifier(penalty='l2', eta0=0.1, n_iter=10, learning_rate='invscaling', n_jobs=-1, alpha=0.0001, loss='hinge')#penalty='l2', eta0=0.1, n_iter=10, learning_rate='invscaling', n_jobs=-1, alpha=0.0001, loss='hinge'\n",
    "clf.fit(X_pca, y_train)\n",
    "y_pred_train = clf.predict(X_pca)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"SGD / training test / pca nf= %d / score= %0.3f\"%(my_ncomp, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did not manage to understand how LDA works... I even  think there might be a problem, because for n_comp = 0, I still had the very same score.\n",
    "\n",
    "So as for PCA, it does decrease a lot the score on the previous best estimator, when dimensionality is reduced... But it is not much of a surprise.  \n",
    "\n",
    "The array printed before shows the realtive importance of the different parameters. As The most significant parameter has a variance ratio of 31 (%1e5), maybe it is safe to consider only features that have more than 6(%e15) variance ratio. That corresponds to 74 features : **roughly 60%** of he original set.\n",
    "\n",
    "# Step 4\n",
    "Now let us use pca simplification to compute a SVC. It takes so much time, that we will also train it on a subset of the global data\n",
    "## SVC + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently training the SVC with following parameters : \n",
      " PCA reduction ratio= 100 \n",
      " Training Sample Ratio= 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-8e5220d3628c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_svc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_svc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total timed used for fitting: %0.3f s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#ratio of training data among the total set available that will be used for fitting the SVC classifiers\n",
    "ratio_sd = 50 #as a percentage\n",
    "n_sd = X_train.shape[0]*ratio_sd/100\n",
    "X_train_svc, y_train_svc = shuffle(X_pca, y_train, n_samples=n_sd )\n",
    "\n",
    "#Simple SVC\n",
    "#X_train_svc.flags['C_CONTIGUOUS'] # check True to improve the speed of the algorithm\n",
    "print(\"Currently training the SVC with following parameters : \\n PCA reduction ratio= %d \\n Training Sample Ratio= %d\"%(ratio_dr, ratio_sd))\n",
    "clf = SVC(kernel='rbf', gamma='auto', cache_size=2500)\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(X_train_svc, y_train_svc)\n",
    "print(\"total timed used for fitting: %0.3f s\"%(time.time() - start))\n",
    "\n",
    "y_pred_train = clf.predict(X_pca)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"default SVC / training test / pca nf= %d / score= %0.3f\"%(my_ncomp, score))\n",
    "print(\"\\n\\nClassification report\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is the best estimator so far, let us perform a grid search to make sure we get the best of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 3692.115 s\n",
      "Tuned / training test reduced to 52800 samples / pca nf= 77 / score= 0.166 for params= {'gamma': 'auto', 'cache_size': 1000, 'kernel': 'rbf', 'C': 1}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'display' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-771c893cb53e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\nClassification report for the best estimator\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'display' is not defined"
     ]
    }
   ],
   "source": [
    "#ratio of training data among the total set available that will be used for fitting the SVC classifiers\n",
    "ratio_sd = 50 #as a percentage\n",
    "n_sd = X_train.shape[0]*ratio_sd/100\n",
    "X_train_svc, y_train_svc = shuffle(X_pca, y_train, n_samples=n_sd )\n",
    "\n",
    "#definining the search parameters\n",
    "svc_tuned_parameters = [{'kernel': ['rbf'], \n",
    "                         'gamma': [.1, 10, 1000, 'auto'],\n",
    "                         'C': [1],\n",
    "                         'cache_size' : [1000],\n",
    "                        }]\n",
    "\n",
    "#Performing the gridsearch\n",
    "clf = GridSearchCV(SVC(shrinking=True), param_grid= svc_tuned_parameters, n_jobs= 4, cv=3)\n",
    "start = time.time()\n",
    "clf.fit(X_train_svc, y_train_svc)\n",
    "print(\"total time used for GridSearch fitting: %0.3f s\"%(time.time() - start))\n",
    "y_pred_train = clf.predict(X_pca)\n",
    "\n",
    "#Score and detailed feedback\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Tuned / training test reduced to %d samples / pca nf= %d / score= %0.3f for params= %a\"%( n_sd, my_ncomp, score, clf.best_params_))\n",
    "\n",
    "results = pd.DataFrame(clf.cv_results_)\n",
    "print(results)\n",
    "\n",
    "print(\"\\n\\nClassification report for the best estimator\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"\\n\\nConfusion matrix for the best estimator\")\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the ideal gridsearch below would be way too time-consuming, \n",
    "```python\n",
    "svc_tuned_parameters = [{'kernel': ['rbf'], \n",
    "                         'gamma': [1e-3, 1e-4, 'auto'],\n",
    "                         'C': [0.01, 0.1, 1, 10],\n",
    "                         'cache_size' : [1000],\n",
    "                        },\n",
    "                        {'kernel': ['poly'],\n",
    "                         'C': [0.1, 1, 10],\n",
    "                         'deg': [3, 5],\n",
    "                         'cache_size' : [1000],\n",
    "                        }]\n",
    "```\n",
    "I had to adapt.  \n",
    "\n",
    "I started with default parameters, and tried to choose gamma (within a range like logspace (1e-3, 1e3)). Obviously, the best result is always obtained for *'auto'*, which, according to the documentation corresponds to $\\frac{1}{n_{sd}}$\n",
    "\n",
    "Now let us try to select C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 8092.161 s\n",
      "Tuned / training test reduced to 52800 samples / pca nf= 77 / score= 0.116 for params= {'gamma': 'auto', 'C': 100, 'kernel': 'rbf', 'cache_size': 1000}\n",
      "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
      "0    6804.517463        95.351618         0.500152          0.500152  0.0001   \n",
      "1    2615.046944        30.464952         0.962481          0.967472     0.1   \n",
      "2      63.202128        16.544076         0.976742          0.991487       1   \n",
      "3      71.447156        16.906226         0.977008          0.999991     100   \n",
      "4      70.249998        16.874936         0.977008          0.999991   10000   \n",
      "\n",
      "  param_cache_size param_gamma param_kernel  \\\n",
      "0             1000        auto          rbf   \n",
      "1             1000        auto          rbf   \n",
      "2             1000        auto          rbf   \n",
      "3             1000        auto          rbf   \n",
      "4             1000        auto          rbf   \n",
      "\n",
      "                                              params  rank_test_score  \\\n",
      "0  {'gamma': 'auto', 'C': 0.0001, 'kernel': 'rbf'...                5   \n",
      "1  {'gamma': 'auto', 'C': 0.1, 'kernel': 'rbf', '...                4   \n",
      "2  {'gamma': 'auto', 'C': 1, 'kernel': 'rbf', 'ca...                3   \n",
      "3  {'gamma': 'auto', 'C': 100, 'kernel': 'rbf', '...                1   \n",
      "4  {'gamma': 'auto', 'C': 10000, 'kernel': 'rbf',...                1   \n",
      "\n",
      "   split0_test_score  split0_train_score  split1_test_score  \\\n",
      "0           0.500142            0.500156           0.500170   \n",
      "1           0.962104            0.967357           0.962727   \n",
      "2           0.976990            0.991363           0.975795   \n",
      "3           0.976138            1.000000           0.976932   \n",
      "4           0.976138            1.000000           0.976932   \n",
      "\n",
      "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
      "0            0.500142           0.500142            0.500156    362.174543   \n",
      "1            0.967869           0.962612            0.967188   3550.524588   \n",
      "2            0.991733           0.977442            0.991364      1.788218   \n",
      "3            0.999972           0.977953            1.000000      0.225939   \n",
      "4            0.999972           0.977953            1.000000      0.124542   \n",
      "\n",
      "   std_score_time  std_test_score  std_train_score  \n",
      "0        3.555003        0.000013         0.000007  \n",
      "1        1.610189        0.000270         0.000290  \n",
      "2        0.083026        0.000695         0.000174  \n",
      "3        0.186291        0.000743         0.000013  \n",
      "4        0.331175        0.000743         0.000013  \n",
      "\n",
      "\n",
      "Classification report for the best estimator\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.99      0.99      0.99     52800\n",
      "          1       0.99      0.99      0.99     52800\n",
      "\n",
      "avg / total       0.99      0.99      0.99    105600\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix for the best estimator\n",
      "[[52155   645]\n",
      " [  578 52222]]\n"
     ]
    }
   ],
   "source": [
    "#definining the search parameters\n",
    "svc_tuned_parameters = [{'kernel': ['rbf'], \n",
    "                         'gamma': ['auto'],\n",
    "                         'C': [0.0001, 0.1, 1, 100, 1000, 10000],\n",
    "                         'cache_size' : [1000],\n",
    "                        }]\n",
    "\n",
    "#Performing the gridsearch\n",
    "clf = GridSearchCV(SVC(shrinking=True), param_grid= svc_tuned_parameters, n_jobs= 4, cv=3)\n",
    "start = time.time()\n",
    "clf.fit(X_train_svc, y_train_svc)\n",
    "print(\"total time used for GridSearch fitting: %0.3f s\"%(time.time() - start))\n",
    "y_pred_train = clf.predict(X_pca)\n",
    "\n",
    "#Score and detailed feedback\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Tuned SVM/ training test reduced to %d samples / pca nf= %d / score= %0.3f for params= %a\"%( n_sd, my_ncomp, score, clf.best_params_))\n",
    "\n",
    "results = pd.DataFrame(clf.cv_results_)\n",
    "print(results)\n",
    "\n",
    "print(\"\\n\\nClassification report for the best estimator\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"\\n\\nConfusion matrix for the best estimator\")\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is frankly awesome : we get a score of 0.116 on the training test with params= {'gamma': 'auto', 'C': 100, 'kernel': 'rbf', 'cache_size': 1000}. Never performed so good before. I just should pay attention to overfitting, but it looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 2052.918 s\n",
      "default SVC / training test / pca nf= 77 / score= 0.116\n",
      "\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.99      0.99      0.99     52800\n",
      "          1       0.99      0.99      0.99     52800\n",
      "\n",
      "avg / total       0.99      0.99      0.99    105600\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[52155   645]\n",
      " [  578 52222]]\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', gamma='auto', C=100, cache_size=2500, probability=True)\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(X_train_svc, y_train_svc)\n",
    "print(\"total timed used for fitting: %0.3f s\"%(time.time() - start))\n",
    "\n",
    "y_pred_train = clf.predict(X_pca)\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Optimal SVC / training test / pca nf= %d / score= %0.3f\"%(my_ncomp, score))\n",
    "print(\"\\n\\nClassification report\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here comes the label 0\n",
    "Improvement of the score by attribution of the label '0' when probability of prediction in one of the classes is too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class = 1 with probaset = [[ 0.00478337  0.99521663]]\n",
      "predicted classes = [1 1 1 1 1] with probaset = [[ 0.01570091  0.98429909]\n",
      " [ 0.01096714  0.98903286]\n",
      " [ 0.04554413  0.95445587]\n",
      " [ 0.01569454  0.98430546]\n",
      " [ 0.38671378  0.61328622]]\n",
      "[ 0.00478337  0.99521663] [[ 0.01570091  0.98429909]\n",
      " [ 0.01096714  0.98903286]\n",
      " [ 0.04554413  0.95445587]\n",
      " [ 0.01569454  0.98430546]\n",
      " [ 0.38671378  0.61328622]]\n",
      "Optimal SVC / training test / pca nf= 77 / WITHOUT 0 processing score= 0.116\n",
      "Optimal SVC / training test / pca nf= 77 / WITH 0 processing score= 0.083\n"
     ]
    }
   ],
   "source": [
    "#Test on single value :\n",
    "test = X_pca[0].reshape(1,-1)\n",
    "print(\"predicted class = %d with probaset = %s\" % (clf.predict(test),clf.predict_proba(test) ))\n",
    "\n",
    "#Test on a larger scale:\n",
    "test = X_pca[10:15]\n",
    "print(\"predicted classes = %s with probaset = %s\" % (clf.predict(test),clf.predict_proba(test) ))\n",
    "\n",
    "#Full scale probability set\n",
    "prediction_set = clf.predict_proba(X_pca) #For each index of X_pca, the proba that it belongs to class -1 and then 1\n",
    "print(prediction_set[0], prediction_set[10:15])\n",
    "\n",
    "#Adaptating the classifier\n",
    "y_pred_train_with0 = np.zeros(y_pred_train.shape) #just to initialize the size\n",
    "y_pred_train_with0[prediction_set[:,0] > 0.85] = -1 #if we know that more that 85% that it is class -1\n",
    "y_pred_train_with0[prediction_set[:,1] > 0.85] = 1  #if we know that more that 85% that it is class 1\n",
    "#The other values are already set to 0 (less than 85% chance that it belongs to one class or the other)\n",
    "\n",
    "#Comparing the score with and without this postprocessing\n",
    "score = compute_pred_score(y_train, y_pred_train)\n",
    "print(\"Optimal SVC / training test / pca nf= %d / WITHOUT 0 processing score= %0.3f\"% (my_ncomp, score))\n",
    "score = compute_pred_score(y_train, y_pred_train_with0)\n",
    "print(\"Optimal SVC / training test / pca nf= %d / WITH 0 processing score= %0.3f\"% (my_ncomp, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying this processus with the test data and sending it online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adapting the test set\n",
    "XX = pca.transform(X_test)\n",
    "prediction_set = clf.predict_proba(XX) #For each index of XX, the proba that it belongs to class -1 and then 1\n",
    "\n",
    "#Adaptating the classifier\n",
    "y_pred_test_with0 = np.zeros(XX.shape[0]) #just to initialize the size\n",
    "y_pred_test_with0[prediction_set[:,0] > 0.7] = -1\n",
    "y_pred_test_with0[prediction_set[:,1] > 0.7] = 1\n",
    "#The other values are already set to 0\n",
    "\n",
    "#saving results to a textfile\n",
    "np.savetxt('y_pred2.txt', y_pred_test_with0, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F\\*\\*\\* ! It is still a \"low\" score of 0.3 on the test set... \n",
    "As I don't want to spend more time fine-tuning this estimator, or trying ensemble methods with it, because it has a long computing time, I will try other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude\n",
    "\n",
    "## Making sure we have the appropriate datasets\n",
    "As I tried many things before, I want to make sure that we start this section with a clean, full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "\n",
      " explained variance ratio as a 'per thousand' ratio for each of the selected features\n",
      "[ 30.  23.  22.  21.  21.  20.  20.  19.  19.  18.  18.  18.  17.  17.  16.\n",
      "  16.  16.  15.  15.  15.  14.  14.  14.  13.  13.  12.  12.  12.  12.  12.\n",
      "  11.  11.  11.  11.  10.  10.  10.  10.  10.  10.   9.   9.   9.   9.   9.\n",
      "   9.   8.   8.   8.   8.   8.   8.   8.   8.   8.   7.   7.   7.   7.   7.\n",
      "   7.   7.   6.   6.   6.   6.   6.   6.   6.   6.   6.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   4.   4.   4.   4.   4.   4.   4.   4.   4.\n",
      "   4.   4.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   2.\n",
      "   2.   2.   2.   2.   2.   2.   2.   2.   2.   1.   1.   1.   1.   1.   1.\n",
      "   1.   1.   1.   1.   1.   1.]\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    }
   ],
   "source": [
    "############### Standardization of the data\n",
    "myScaler = StandardScaler()\n",
    "X_train_scaled = myScaler.fit_transform(X_train)\n",
    "X_test_scaled = myScaler.transform(X_test)\n",
    "\n",
    "############### PCA : manual procedure\n",
    "# ratio_dr = 100 # dimensionality reduction ratio as a percentage\n",
    "# my_ncomp = round(X_train.shape[1]*ratio_dr/100) # effective number of features retained\n",
    "# print(\"%d features selected out of %d (%d %%) for PCA\"%(my_ncomp, X_train.shape[1], ratio_dr))\n",
    "\n",
    "# #PCA in effect with manual selection\n",
    "# pca_scaled = PCA(svd_solver='full', whiten=True, n_components=my_ncomp).fit(X_train_scaled)\n",
    "# X_pca_scaled = pca_scaled.transform(X_train_scaled)\n",
    "\n",
    "############### PCA : automatic procedure\n",
    "#select the variance ratio such that the number of components explains at least this ratio. \n",
    "var_ratio_min = 99.9 #as a percentage (float number 0<num<100 strictly)\n",
    "\n",
    "#PCA in effect with automatic selection of variables to keep\n",
    "pca_scaled = PCA(svd_solver='full', whiten=True, n_components=var_ratio_min/100).fit(X_train_scaled)\n",
    "X_pca_scaled = pca_scaled.transform(X_train_scaled)\n",
    "print(\"%d features selected out of %d (%d %%) for PCA which explains %d %% of variance\"%(pca_scaled.n_components_, X_train.shape[1], pca_scaled.n_components_/X_train.shape[1]*100, pca_scaled.explained_variance_ratio_.sum()*100))\n",
    "\n",
    "#Explained variance ratio\n",
    "print(\"\\n explained variance ratio as a 'per thousand' ratio for each of the selected features\")\n",
    "print((pca_scaled.explained_variance_ratio_*1000).round())\n",
    "\n",
    "################ Observation Selection and Mixing\n",
    "#ratio of training data among the total set available that will be used for fitting the SVC classifiers\n",
    "ratio_sd = 100 #as a percentage\n",
    "n_sd = X_train.shape[0]*ratio_sd/100 #effective number of observations retained\n",
    "print(\"%d observations selected out of %d (%d %%) for Shuffling and training\"%(n_sd, X_train.shape[0], ratio_sd))\n",
    "\n",
    "X_train_scaled_shuffled, y_train_scaled_shuffled = shuffle(X_pca_scaled, y_train, n_samples=n_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let me sum up what we've got so far :\n",
    "+ ** X_train, X_test, y_train, y_test** : the original dataset\n",
    "+ ** X_train_scaled, , X_test** : the standardized dataset, obtained with *myScaler* trained on X_train\n",
    "+ ** X_pca** : the result of the pca with *my_ncomp* components on the training set: use *pca.transform()* to replicate transformation\n",
    "+ ** X_pca_scaled** : the result of the pca with all the components obtained with *X_train_scaled*  : use *pca_scaled.transform()* to replicate transformation\n",
    "+ ** X_train_scaled_shuffled, y_train_scaled_shuffled** : standardization + pca + shuffle\n",
    "\n",
    "I will try to use the last one as it is supposed to be the most appropriate to give all most significant variables obtained with PCA the same importance (standardization), and should not depend on the order of the objects wisited (shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am growing fed up of repeating over and over the same processes, let me introduce a few fuctions we will use later\n",
    "## A few useful homemade functions\n",
    "+ If computing time is long, or if there is overfitting, we might want to train the algorithm on a smaller sample, that is to say less variables, and less observations at the same time.\n",
    "+ On top of that we want to be able to easily adapt the 0_labels technique\n",
    "+ Finally we want to save quickly the resulting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTimeSignificant(t_seconds):\n",
    "    #transforms seconds into hours, minutes, and seconds\n",
    "    m, s = divmod(t_seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%dh%02dm%02ds\" % (h, m, s)\n",
    "\n",
    "def predict_0_labels(XX, clf, threshold=0.7, without=False):\n",
    "    #Add the 0 labels to a prediction to increase score\n",
    "    #check whether the classifier is compatible\n",
    "    can_predict_proba = getattr(clf, \"predict_proba\", None)\n",
    "    if callable(can_predict_proba):\n",
    "        print(\"0_labels enabled\")\n",
    "        start = time.time()\n",
    "        prediction_set = clf.predict_proba(XX) #For each index of XX, the proba that it belongs to class -1 and then 1\n",
    "        print(\"total timed used for predicting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "        #Adaptating the classifier\n",
    "        y_pred_with0 = np.zeros(XX.shape[0]) #just to initialize the size\n",
    "        y_pred_with0[prediction_set[:,0] > threshold] = -1\n",
    "        y_pred_with0[prediction_set[:,1] > threshold] = 1\n",
    "        #The other values are already set to 0\n",
    "        \n",
    "        if(without):\n",
    "            y_pred = np.ones(XX.shape[0])\n",
    "            y_pred[prediction_set[:,0] >= 0.5] = -1\n",
    "            return y_pred_with0, y_pred\n",
    "        else:\n",
    "            return y_pred_with0\n",
    "    else:\n",
    "        print(\"0_labels disabled\")\n",
    "        start = time.time()\n",
    "        y_pred = clf.predict(XX)\n",
    "        print(\"total timed used for predicting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "        if(without):\n",
    "            return y_pred, y_pred\n",
    "        else:\n",
    "            return y_pred\n",
    "\n",
    "def prepare_dataset(XX_train, y_train, XX_test, var_ratio_min=99.9, ratio_sd=100):\n",
    "    #Scale it \n",
    "    myScaler = StandardScaler()\n",
    "    XX_train_scaled = myScaler.fit_transform(XX_train)\n",
    "\n",
    "    #select the most significant features\n",
    "    pca_scaled = PCA(svd_solver='full', whiten=True, n_components=var_ratio_min/100).fit(XX_train_scaled)\n",
    "    XX_pca_scaled = pca_scaled.transform(XX_train_scaled)\n",
    "    print(\"%d features selected out of %d (%d %%) for PCA which explains %d %% of variance\"%(pca_scaled.n_components_, XX_train.shape[1], pca_scaled.n_components_/XX_train.shape[1]*100, pca_scaled.explained_variance_ratio_.sum()*100))\n",
    "\n",
    "    #print(\"\\n explained variance ratio as a 'per thousand' ratio for each of the selected features\")\n",
    "    #print((pca_scaled.explained_variance_ratio_*1000).round())\n",
    "\n",
    "    #Select a certain amount of observations\n",
    "    n_sd = XX_train.shape[0]*ratio_sd/100 #effective number of observations retained\n",
    "    print(\"%d observations selected out of %d (%d %%) for Shuffling and training\"%(n_sd, XX_train.shape[0], ratio_sd))\n",
    "\n",
    "    #Shuffle it\n",
    "    XX_train_scaled_shuffled, yy_train_scaled_shuffled = shuffle(XX_pca_scaled, y_train, n_samples=n_sd)\n",
    "    \n",
    "    #Adapt the test set accordingly\n",
    "    XX_test_scaled = myScaler.transform(XX_test)\n",
    "    XX_test_scaled_pca = pca_scaled.transform(XX_test_scaled)\n",
    "    \n",
    "    return XX_train_scaled_shuffled, yy_train_scaled_shuffled, XX_test_scaled_pca\n",
    "\n",
    "def save_prediction(X_test, clf, trial_number, threshold=0.7):\n",
    "    y_pred = predict_0_labels(X_test, clf, threshold=threshold)\n",
    "    np.savetxt('y_pred_' + str(trial_number) + '.txt', y_pred, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5\n",
    "Nearest Neighbours classification\n",
    "## K-neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 2.711 s\n",
      "Default K-neighbours classifier/ training test / pca nf= 126 / score= 0.220\n",
      "\n",
      "\n",
      "Classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.97      0.99      0.98     52800\n",
      "          1       0.99      0.97      0.98     52800\n",
      "\n",
      "avg / total       0.98      0.98      0.98    105600\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[52233   567]\n",
      " [ 1758 51042]]\n"
     ]
    }
   ],
   "source": [
    "#the number of neighbours in the k-neighbours algorithm\n",
    "K = 5 #5 is the default value. If it runs fast, gridsearch can be intempted with many different values\n",
    "weight_scheme = 'uniform' # could be inverse of 'distance'\n",
    "leafSize = 30\n",
    "#Trying the algorithm on scaled data\n",
    "clf = KNeighborsClassifier(n_neighbors=K, \n",
    "                           weights= weight_scheme,  \n",
    "                           leaf_size= leafSize, \n",
    "                           algorithm='ball_tree', metric='minkowski', p=2,\n",
    "                           n_jobs=4)\n",
    "\n",
    "start = time.time()\n",
    "clf.fit(X_train_scaled_shuffled, y_train_scaled_shuffled)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "start = time.time()\n",
    "y_pred_train = clf.predict(X_train_scaled_shuffled)\n",
    "print(\"total timed used for predicting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "score = compute_pred_score(y_train_scaled_shuffled, y_pred_train)\n",
    "print(\"Default K-neighbours classifier/ training test / pca nf= %d / score= %0.3f\"%(pca_scaled.n_components_, score))\n",
    "print(\"\\n\\nClassification report\")\n",
    "print(classification_report(y_train_scaled_shuffled, y_pred_train))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_scaled_shuffled, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because it took the hell of a long time, I will try a smaller dataset, and better parameters for quicker analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 features selected out of 128 (76 %) for PCA which explains 95 % of variance\n",
      "95040 observations selected out of 105600 (90 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 0:00:01\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:10:25\n",
      "K-Neighbours / training test ns=95040 / pca nf= 98 / score= 0.117\n",
      "now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:54\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=95, ratio_sd=90)\n",
    "\n",
    "#the number of neighbours in the k-neighbours algorithm\n",
    "K = 10 #5 is the default value. Big values reduce overfitting\n",
    "weight_scheme = 'uniform' # could be inverse of 'distance'\n",
    "leafSize = 50\n",
    "clf = KNeighborsClassifier(n_neighbors=K, \n",
    "                           weights= weight_scheme,  \n",
    "                           leaf_size= leafSize, \n",
    "                           algorithm='ball_tree', metric='minkowski', p=2,\n",
    "                           n_jobs=4)\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train_with0 = predict_0_labels(X_train_adapt, clf)\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train_with0)\n",
    "print(\"K-Neighbours / training test ns=%d / pca nf= %d / score= %0.3f\"%(X_train_adapt.shape[0], X_train_adapt.shape[1], score))\n",
    "# print(\"\\n\\nConfusion matrix\")\n",
    "# print(confusion_matrix(y_train_scaled_shuffled, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"now for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oh my god it takes so much time !!!** I am changing my classifier illico presto. The score on the training set is not too bad though ! Again, I am facing overfitting. Because the score obtained for the test set is only 0.4...\n",
    "\n",
    "I will come back to this method later, because I don't want to loose to much time optimizing a technique that might reveal itself suboptimal later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 0:00:00\n",
      "total timed used for predicting: 0:00:00\n",
      "NearestCentroid / training test / pca nf= 126 / score= 0.646\n",
      "\n",
      "Confusion matrix\n",
      "[[49383  3417]\n",
      " [ 3410 49390]]\n",
      "\n",
      "Now for the test set\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "\n",
    "clf = NearestCentroid()\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(X_train_adapt, clf)\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"NearestCentroid / training test ns=%d / pca nf= %d / score= %0.3f\"%(X_train_adapt.shape[0], X_train_adapt.shape[1], score))\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a terrible score when compared to the best estimators found before.\n",
    "\n",
    "Let us move on to a new technique :\n",
    "\n",
    "# Step 6\n",
    "## Gaussian processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 features selected out of 128 (76 %) for PCA which explains 95 % of variance\n",
      "73920 observations selected out of 105600 (70 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-81ed648644c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m#Fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_adapt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_adapt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"total timed used for fitting: %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmakeTimeSignificant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpc.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    608\u001b[0m                                  % self.multi_class)\n\u001b[1;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpc.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_marginal_likelihood_value_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_marginal_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[1;31m# Precompute quantities required for predictions which are independent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\gpc.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[1;31m# Compute log-marginal-likelihood Z and also store some temporaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m    756\u001b[0m                                        K2_gradient * K1[:, :, np.newaxis]))\n\u001b[1;32m    757\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gradient can only be evaluated when Y is None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameter_constant_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "\n",
    "clf = GaussianProcessClassifier(n_jobs=4)\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train_with0 = predict_0_labels(X_train_adapt, clf)\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train_with0)\n",
    "print(\"GaussianProcessClassifier / training test ns=%d / pca nf= %d / score= %0.3f\"%(X_train_adapt.shape[0], X_train_adapt.shape[1], score))\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train_with0))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7\n",
    "## Naive Bayes methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 0:00:00\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n",
      "NearestCentroid / training test ns=105600 / pca nf= 126 / score= 1.121\n",
      "\n",
      "Confusion matrix\n",
      "[[42430  4770  5600]\n",
      " [    0     0     0]\n",
      " [ 5125  6342 41333]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "\n",
    "clf = GaussianNB(priors= [0.5, 0.5])\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train_with0 = predict_0_labels(X_train_adapt, clf)\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train_with0)\n",
    "print(\"GaussianNB / training test ns=%d / pca nf= %d / score= %0.3f\"%(X_train_adapt.shape[0], X_train_adapt.shape[1], score))\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train_with0))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is not a satisfactory result\n",
    "## BernouilliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 0:00:00\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n",
      "NearestCentroid / training test / pca nf= 126 / score= 0.857\n",
      "\n",
      "Confusion matrix\n",
      "[[27951 22679  2170]\n",
      " [    0     0     0]\n",
      " [ 2382 22294 28124]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "\n",
    "clf = BernoulliNB(class_prior= [0.5, 0.5])\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train_with0 = predict_0_labels(X_train_adapt, clf, threshold=0.7)\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train_with0)\n",
    "print(\"BernouilliNB / training test ns= %d/ pca nf= %d / score= %0.3f\"%(X_train_adapt.shape[0], X_train_adapt.shape[1], score))\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train_with0))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is still quite unsatisfactory\n",
    "# Step 8\n",
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 features selected out of 128 (66 %) for PCA which explains 90 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for fitting: 0:00:16\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n",
      "DecisionTree / training test ns=105600 / pca nf= 85 / score= 0.734\n",
      "\n",
      "Confusion matrix\n",
      "[[41158  7860  3782]\n",
      " [    0     0     0]\n",
      " [ 2397  7878 42525]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=90, ratio_sd=100)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=15, min_samples_split=100 )\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total timed used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train_with0 = predict_0_labels(X_train_adapt, clf, threshold=0.7)\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train_with0)\n",
    "print(\"DecisionTree / training test ns=%d / pca nf= %d / score= %0.3f\"%(X_train_adapt.shape[0], X_train_adapt.shape[1], score))\n",
    "print(\"\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train_with0))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I will not perform Gridsearch just for the parameters of the trees as it is easy to find trees that just perfectly fit the data...But amazingly overfits it (the eight prediction test set I uploaded scored more than 2 !!!)\n",
    "\n",
    "This is why I will use a pipe, in order to find the best amount of features that can be used to train the data, and also the best tree_depth based on a gridsearch that takes both into account. I will make cross-validation significant : cv=10\n",
    "\n",
    "Actually, I could have done that before with SVC, but as SVC take a long time to compute, I left it the way I had it. It is more doable with decision trees which take each something like 16s, as opposed to SVC which take 8m\n",
    "\n",
    "## Cross-validated  trees + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 0:08:31\n",
      "\n",
      "============best params found {'reduce_dim__n_components': 126, 'classify__min_samples_split': 2, 'classify__splitter': 'best', 'reduce_dim': PCA(copy=True, iterated_power='auto', n_components=126, random_state=None,\n",
      "  svd_solver='full', tol=0.0, whiten=False), 'classify__max_depth': 14}\n",
      "\n",
      "============Classification report\n",
      "    rank test  score test  score train PCA ratio_f max_depth min sample split  \\\n",
      "13          1    0.734924     0.928369         100        14                2   \n",
      "5           2    0.734100     0.901165         100        13                5   \n",
      "21          3    0.733551     0.923894         100        14                7   \n",
      "1           4    0.733523     0.902384         100        13                2   \n",
      "17          5    0.733485     0.926458         100        14                5   \n",
      "12          5    0.733485     0.929744      97.619        14                2   \n",
      "0           7    0.733116     0.903608      97.619        13                2   \n",
      "9           7    0.733116     0.899470         100        13                7   \n",
      "16          9    0.732481     0.927708      97.619        14                5   \n",
      "20         10    0.732358     0.924941      97.619        14                7   \n",
      "4          11    0.732064     0.902327      97.619        13                5   \n",
      "8          12    0.732027     0.900537      97.619        13                7   \n",
      "11         13    0.717415     0.821191         100        13                7   \n",
      "3          14    0.717197     0.819789         100        13                2   \n",
      "2          15    0.716695     0.818790      97.619        13                2   \n",
      "22         16    0.716383     0.837907      97.619        14                7   \n",
      "19         17    0.715824     0.842874         100        14                5   \n",
      "23         18    0.715294     0.833665         100        14                7   \n",
      "14         19    0.715256     0.842214      97.619        14                2   \n",
      "18         20    0.714081     0.854212      97.619        14                5   \n",
      "10         21    0.713617     0.813196      97.619        13                7   \n",
      "6          22    0.713059     0.814815      97.619        13                5   \n",
      "7          23    0.712983     0.806631         100        13                5   \n",
      "15         24    0.711752     0.852794         100        14                2   \n",
      "\n",
      "   splitter  \n",
      "13     best  \n",
      "5      best  \n",
      "21     best  \n",
      "1      best  \n",
      "17     best  \n",
      "12     best  \n",
      "0      best  \n",
      "9      best  \n",
      "16     best  \n",
      "20     best  \n",
      "4      best  \n",
      "8      best  \n",
      "11   random  \n",
      "3    random  \n",
      "2    random  \n",
      "22   random  \n",
      "19   random  \n",
      "23   random  \n",
      "14   random  \n",
      "18   random  \n",
      "10   random  \n",
      "6    random  \n",
      "7    random  \n",
      "15   random  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:41: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Defining the pipe predictor\n",
    "pipe = Pipeline([ ('reduce_dim', PCA()), ('classify', DecisionTreeClassifier())])\n",
    "\n",
    "#Selecting parameters for grid-search\n",
    "N_FEATURES_RATIOS = np.array([98, 99.9]) #as a percentage\n",
    "N_FEATURES_OPTIONS = np.round(X_train_adapt.shape[1]*N_FEATURES_RATIOS/100).astype('int').tolist()\n",
    "DEPTH_OPTIONS = [13, 14]\n",
    "SPLIT_OPTIONS = [2, 5, 7]\n",
    "SPLITTER_OPTIONS = ['best', 'random']\n",
    "\n",
    "param_grid = [{'reduce_dim': [PCA(svd_solver='full')],\n",
    "               'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "               'classify__max_depth': DEPTH_OPTIONS,\n",
    "               'classify__min_samples_split' : SPLIT_OPTIONS,\n",
    "               'classify__splitter' : SPLITTER_OPTIONS\n",
    "              }]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=4)\n",
    "\n",
    "#Fitting the grid\n",
    "start = time.time()\n",
    "grid.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for GridSearch fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_reduce_dim__n_components',\n",
    "                                          'param_classify__max_depth', \n",
    "                                          'param_classify__min_samples_split',\n",
    "                                          'param_classify__splitter'\n",
    "                                         ]]\n",
    "results['param_reduce_dim__n_components'] = results['param_reduce_dim__n_components']/126*100\n",
    "results.columns = ['rank test', 'score test', 'score train', 'PCA ratio_f', 'max_depth', 'min sample split', 'splitter']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal param_grid (my computer remained idle, I don't know why. Afraid of the work ahead ?), that I split into several sets to try to get the best out of it\n",
    "```python\n",
    "#Selecting parameters for grid-search\n",
    "N_FEATURES_RATIOS = np.array([85, 90, 95, 99.9]) #as a percentage\n",
    "N_FEATURES_OPTIONS = np.round(X_train_adapt.shape[1]*N_FEATURES_RATIOS/100).astype('int').tolist()\n",
    "DEPTH_OPTIONS = np.arange(10, 20, 3)\n",
    "SPLIT_OPTIONS = np.arange(1, 100, 20)\n",
    "```\n",
    "\n",
    "After a long study, it seems that it is always best to keep most of the features, and that optimal tree_depth is around 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 0:09:46\n",
      "\n",
      "============best params found {'reduce_dim__n_components': 126, 'classify__min_samples_split': 3, 'classify__splitter': 'best', 'classify__max_depth': 14}\n",
      "\n",
      "============Classification report\n",
      "    rank test  score test  score train PCA ratio_f max_depth min sample split\n",
      "4           1    0.735502     0.920188         100        14                3\n",
      "5           2    0.735161     0.919445         100        14                4\n",
      "3           3    0.734650     0.920244         100        14                2\n",
      "0           4    0.734574     0.941483         100        15                2\n",
      "1           5    0.734394     0.940354         100        15                4\n",
      "7           6    0.733295     0.893483         100        13                4\n",
      "6           7    0.733191     0.911920         100        14               10\n",
      "9           8    0.733011     0.892129         100        13                6\n",
      "2           9    0.732945     0.935894         100        15                7\n",
      "8          10    0.732481     0.892852         100        13                5\n",
      "10         11    0.732330     0.891161         100        13                7\n",
      "11         12    0.731458     0.888248         100        13               10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:37: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Further customizing\n",
    "param_grid = [{'reduce_dim__n_components': [126],\n",
    "               'classify__max_depth': [15],\n",
    "               'classify__min_samples_split' : [2, 4, 7],\n",
    "               'classify__splitter' : ['best']\n",
    "              },\n",
    "              {'reduce_dim__n_components': [126],\n",
    "               'classify__max_depth': [14],\n",
    "               'classify__min_samples_split' : [2,3,4, 10],\n",
    "               'classify__splitter' : ['best']\n",
    "              },\n",
    "              {'reduce_dim__n_components': [126],\n",
    "               'classify__max_depth': [13],\n",
    "               'classify__min_samples_split' : [4,5,6,7, 10],\n",
    "               'classify__splitter' : ['best']\n",
    "              }]\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=6, n_jobs=4)\n",
    "\n",
    "#Fitting the grid\n",
    "start = time.time()\n",
    "grid.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for GridSearch fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_reduce_dim__n_components',\n",
    "                                          'param_classify__max_depth', \n",
    "                                          'param_classify__min_samples_split',\n",
    "                                         ]]\n",
    "results['param_reduce_dim__n_components'] = results['param_reduce_dim__n_components']/126*100\n",
    "results.columns = ['rank test', 'score test', 'score train', 'PCA ratio_f', 'max_depth', 'min sample split']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will therefore keep this last optimal set of values for next PCA. Basically, max_depth=14, min_samples_split=3.\n",
    "As Test score is not very high anyway, we will try to use ensemble methods on trees to get a better result later, and do not save the results just yet. But the trees used for Bagging or Extratrees will be based on this particular study.\n",
    "# Step 9\n",
    "A few ensemble methods based on trees\n",
    "## Extra-trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0:00:07\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n",
      "Score with ExtraTrees estimator 0.188\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[42853  9823   124]\n",
      " [    0     0     0]\n",
      " [   90  7879 44831]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Parameters as defined by the previous study... \n",
    "#We might however want to reduce the depth\n",
    "#change the number of estimators\n",
    "\n",
    "#clf = ExtraTreesClassifier(max_depth=16, n_estimators=70, min_samples_split=3, bootstrap=True, n_jobs=4)\n",
    "clf = ExtraTreesClassifier(max_depth=None, n_estimators=70, min_samples_split=10, min_samples_leaf=10, bootstrap=True, max_features=round(126**.5), n_jobs=4)\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.6)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with ExtraTrees estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite playing  with the data, it did not lead to a good result, therefore i will directly try something else.\n",
    "It overfits way too much or is not accurate (I get more than 0.8) It is a very fast technique though\n",
    "## GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0:12:31\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n",
      "Score with Gradient Boosting estimator 0.158\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[47570  4779   451]\n",
      " [    0     0     0]\n",
      " [  322  4153 48325]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Parameters as defined by the previous study... \n",
    "#We might however want to reduce the depth\n",
    "#change the number of estimators\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=6)\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.7)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with Gradient Boosting estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 16m, we obtain for GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=7) a train score of 0.088 and a test score of 0.33: not too shabby. It is the best estimato obtained so far\n",
    "\n",
    "With (n_estimators=70, learning_rate=0.1, max_depth=10), after 36m, the training score is 0.008, but there is overfitting, because the testscore is 0.38.\n",
    "\n",
    "With (n_estimators=100, learning_rate=0.1, max_depth=5), after 8m trainscore is 0.237, and test_score is >0.46  \n",
    "With (n_estimators=100, learning_rate=0.01, max_depth=7), after 17m, trainscore is 0.646, and test_score is not tested  \n",
    "With (n_estimators=100, learning_rate=0.1, max_depth=6), after 13m, trainscore is 0.158, and test score is 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0:09:04\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:01\n",
      "Score with RandomForestClassifier estimator 0.333\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[41710  9457  1633]\n",
      " [    0     0     0]\n",
      " [  112  8278 44410]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Parameters as defined by the previous study... \n",
    "#We might however want to reduce the depth\n",
    "#change the number of estimators\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=12, max_features=70 , bootstrap=True, n_jobs=4)\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.7)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with RandomForestClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# step 10\n",
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0:00:29\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n",
      "Score with basic RandomForestClassifier estimator 0.002\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[52731    65     4]\n",
      " [    0     0     0]\n",
      " [    0    54 52746]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Parameters as defined by the previous study... \n",
    "#We might however want to reduce the depth\n",
    "#change the number of estimators\n",
    "clf = MLPClassifier()\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.75)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with basic MLPClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woaw, this is amazing ! \n",
    "It is by far the best classifier with only 0.29 as a score on the test set.\n",
    "\n",
    "Even more amazing, the computing time is ridiculous compared to other methods. Let us try to customize it via gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 0:17:52\n",
      "\n",
      "============best params found {'activation': 'tanh', 'alpha': 0.10000000000000001}\n",
      "\n",
      "============Classification report\n",
      "    rank test  score test  score train  alpha activation\n",
      "7           1    0.981600     0.995559    0.1       tanh\n",
      "12          2    0.981184     0.996941    0.1       relu\n",
      "1           3    0.980256     0.999975  0.001   logistic\n",
      "11          4    0.979242     0.999674  0.001       relu\n",
      "10          5    0.978930     0.999326  1e-05       relu\n",
      "0           6    0.978769     0.999970  1e-05   logistic\n",
      "2           7    0.978655     0.982739    0.1   logistic\n",
      "5           8    0.975492     0.999864  1e-05       tanh\n",
      "6           9    0.974271     0.999828  0.001       tanh\n",
      "13         10    0.934896     0.934786     10       relu\n",
      "8          11    0.932377     0.932898     10       tanh\n",
      "3          12    0.930833     0.931405     10   logistic\n",
      "4          13    0.500000     0.500000   1000   logistic\n",
      "9          13    0.500000     0.500000   1000       tanh\n",
      "14         13    0.500000     0.500000   1000       relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:25: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Further customizing\n",
    "param_grid = [{'alpha' : np.logspace(-5, 3, 5),\n",
    "               'activation' : ['logistic', 'tanh', 'relu']\n",
    "              }]\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(), param_grid=param_grid, cv=6, n_jobs=4)\n",
    "\n",
    "#Fitting the grid\n",
    "start = time.time()\n",
    "grid.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for GridSearch fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_alpha', \n",
    "                                          'param_activation',\n",
    "                                         ]]\n",
    "results.columns = ['rank test', 'score test', 'score train', 'alpha', 'activation']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=grid.best_estimator_, trial_number=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woaw !!!! So far it is the best estimator found with a score on the test set of 0.197..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 89.0min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 102.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 1h42m48.174471616744995\n",
      "\n",
      "============best params found {'batch_size': 400, 'alpha': 0.31622776601683794, 'solver': 'adam', 'activation': 'relu'}\n",
      "\n",
      "============Classification report\n",
      "    rank test  score test  score train      alpha activation solver batch\n",
      "30          1    0.982670     0.995675   0.316228       relu   adam   400\n",
      "49          2    0.982585     0.993897        100       relu  lbfgs   NaN\n",
      "44          3    0.982225     0.990490        100       tanh  lbfgs   NaN\n",
      "10          4    0.981657     0.991764   0.316228       tanh   adam   400\n",
      "29          5    0.981070     0.989219   0.316228       relu    sgd   200\n",
      "28          6    0.980966     0.989567   0.316228       relu   adam   200\n",
      "24          7    0.980047     0.999695  0.0177828       relu   adam   200\n",
      "26          8    0.979981     0.999941  0.0177828       relu   adam   400\n",
      "8           9    0.979735     0.985095   0.316228       tanh   adam   200\n",
      "22         10    0.979489     0.999941      0.001       relu   adam   400\n",
      "48         11    0.979261     0.999972    5.62341       relu  lbfgs   NaN\n",
      "25         12    0.979223     0.994164  0.0177828       relu    sgd   200\n",
      "21         13    0.979034     0.994188      0.001       relu    sgd   200\n",
      "20         14    0.978674     0.999607      0.001       relu   adam   200\n",
      "31         15    0.978419     0.986610   0.316228       relu    sgd   400\n",
      "45         16    0.977812     0.999974      0.001       relu  lbfgs   NaN\n",
      "6          17    0.977765     0.999955  0.0177828       tanh   adam   400\n",
      "47         18    0.977652     0.999974   0.316228       relu  lbfgs   NaN\n",
      "27         19    0.977538     0.987403  0.0177828       relu    sgd   400\n",
      "4          20    0.977462     0.999574  0.0177828       tanh   adam   200\n",
      "46         21    0.977273     0.999974  0.0177828       relu  lbfgs   NaN\n",
      "23         22    0.977036     0.987173      0.001       relu    sgd   400\n",
      "9          23    0.976155     0.980362   0.316228       tanh    sgd   200\n",
      "2          24    0.975710     0.999972      0.001       tanh   adam   400\n",
      "43         25    0.975426     0.999969    5.62341       tanh  lbfgs   NaN\n",
      "0          26    0.975161     0.999962      0.001       tanh   adam   200\n",
      "1          27    0.973826     0.987945      0.001       tanh    sgd   200\n",
      "5          28    0.973381     0.987363  0.0177828       tanh    sgd   200\n",
      "11         29    0.968750     0.974302   0.316228       tanh    sgd   400\n",
      "41         30    0.968617     0.999974  0.0177828       tanh  lbfgs   NaN\n",
      "42         31    0.968333     0.999974   0.316228       tanh  lbfgs   NaN\n",
      "7          32    0.967301     0.975154  0.0177828       tanh    sgd   400\n",
      "40         33    0.967121     0.999974      0.001       tanh  lbfgs   NaN\n",
      "3          34    0.966761     0.975206      0.001       tanh    sgd   400\n",
      "35         35    0.964555     0.966061    5.62341       relu    sgd   400\n",
      "34         36    0.963258     0.964321    5.62341       relu   adam   400\n",
      "33         37    0.948277     0.949250    5.62341       relu    sgd   200\n",
      "32         38    0.945767     0.946844    5.62341       relu   adam   200\n",
      "14         39    0.945502     0.945717    5.62341       tanh   adam   400\n",
      "15         40    0.940530     0.940755    5.62341       tanh    sgd   400\n",
      "13         41    0.936364     0.936598    5.62341       tanh    sgd   200\n",
      "19         42    0.935189     0.935452        100       tanh    sgd   400\n",
      "39         43    0.934763     0.935305        100       relu    sgd   400\n",
      "12         44    0.932443     0.932876    5.62341       tanh   adam   200\n",
      "38         45    0.932093     0.932045        100       relu   adam   400\n",
      "18         46    0.931222     0.931143        100       tanh   adam   400\n",
      "37         47    0.530587     0.530902        100       relu    sgd   200\n",
      "17         48    0.516439     0.516771        100       tanh    sgd   200\n",
      "16         49    0.500000     0.500000        100       tanh   adam   200\n",
      "36         49    0.500000     0.500000        100       relu   adam   200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:33: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Further customizing\n",
    "param_grid = [{'alpha' : np.logspace(-3, 2, 5),\n",
    "               'activation' : ['tanh', 'relu'],\n",
    "               'solver' : ['adam', 'sgd'],\n",
    "               'batch_size' : [200,  400]\n",
    "              },\n",
    "              {'alpha' : np.logspace(-3, 2, 5),\n",
    "               'activation' : ['tanh', 'relu'],\n",
    "               'solver' : ['lbfgs']\n",
    "              }]\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(), param_grid=param_grid, cv=5, n_jobs=4, verbose=100)\n",
    "\n",
    "#Fitting the grid\n",
    "start = time.time()\n",
    "grid.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for GridSearch fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_alpha', \n",
    "                                          'param_activation',\n",
    "                                          'param_solver',\n",
    "                                          'param_batch_size'\n",
    "                                         ]]\n",
    "results.columns = ['rank test', 'score test', 'score train', 'alpha', 'activation', 'solver', 'batch']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0h00m0.029016733169555664\n"
     ]
    }
   ],
   "source": [
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=grid.best_estimator_, trial_number=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n",
      "Fitting 5 folds for each of 34 candidates, totalling 170 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n",
      "[Parallel(n_jobs=4)]: Done   1 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=4)]: Done   2 tasks      | elapsed:   39.8s\n",
      "[Parallel(n_jobs=4)]: Done   3 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 tasks      | elapsed:   48.4s\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done   6 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done   7 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done   8 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done   9 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  11 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done  12 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done  14 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  15 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=4)]: Done  16 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=4)]: Done  18 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done  19 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done  20 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=4)]: Done  21 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  22 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=4)]: Done  23 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done  25 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=4)]: Done  27 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=4)]: Done  28 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=4)]: Done  29 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=4)]: Done  30 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done  31 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=4)]: Done  32 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=4)]: Done  34 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=4)]: Done  35 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=4)]: Done  36 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=4)]: Done  37 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=4)]: Done  38 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=4)]: Done  39 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=4)]: Done  40 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=4)]: Done  41 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=4)]: Done  43 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=4)]: Done  44 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=4)]: Done  45 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=4)]: Done  46 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=4)]: Done  47 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=4)]: Done  48 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=4)]: Done  50 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=4)]: Done  51 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=4)]: Done  52 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=4)]: Done  54 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=4)]: Done  55 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=4)]: Done  56 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=4)]: Done  57 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=4)]: Done  59 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=4)]: Done  60 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=4)]: Done  61 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=4)]: Done  62 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=4)]: Done  63 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=4)]: Done  65 tasks      | elapsed: 11.9min\n",
      "[Parallel(n_jobs=4)]: Done  66 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=4)]: Done  67 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=4)]: Done  69 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=4)]: Done  70 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=4)]: Done  71 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=4)]: Done  72 tasks      | elapsed: 13.3min\n",
      "[Parallel(n_jobs=4)]: Done  73 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=4)]: Done  74 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=4)]: Done  75 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=4)]: Done  76 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed: 14.1min\n",
      "[Parallel(n_jobs=4)]: Done  78 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=4)]: Done  79 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=4)]: Done  80 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=4)]: Done  81 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=4)]: Done  82 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=4)]: Done  83 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=4)]: Done  84 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=4)]: Done  85 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=4)]: Done  86 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=4)]: Done  87 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=4)]: Done  88 tasks      | elapsed: 15.8min\n",
      "[Parallel(n_jobs=4)]: Done  89 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=4)]: Done  91 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=4)]: Done  92 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=4)]: Done  93 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=4)]: Done  94 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=4)]: Done  95 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=4)]: Done  96 tasks      | elapsed: 20.5min\n",
      "[Parallel(n_jobs=4)]: Done  97 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=4)]: Done  98 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=4)]: Done  99 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=4)]: Done 100 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=4)]: Done 101 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=4)]: Done 102 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=4)]: Done 103 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=4)]: Done 104 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed: 28.9min\n",
      "[Parallel(n_jobs=4)]: Done 106 tasks      | elapsed: 30.0min\n",
      "[Parallel(n_jobs=4)]: Done 107 tasks      | elapsed: 30.6min\n",
      "[Parallel(n_jobs=4)]: Done 108 tasks      | elapsed: 31.6min\n",
      "[Parallel(n_jobs=4)]: Done 109 tasks      | elapsed: 31.6min\n",
      "[Parallel(n_jobs=4)]: Done 110 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=4)]: Done 111 tasks      | elapsed: 34.0min\n",
      "[Parallel(n_jobs=4)]: Done 112 tasks      | elapsed: 35.4min\n",
      "[Parallel(n_jobs=4)]: Done 113 tasks      | elapsed: 35.6min\n",
      "[Parallel(n_jobs=4)]: Done 114 tasks      | elapsed: 36.3min\n",
      "[Parallel(n_jobs=4)]: Done 115 tasks      | elapsed: 36.6min\n",
      "[Parallel(n_jobs=4)]: Done 116 tasks      | elapsed: 36.8min\n",
      "[Parallel(n_jobs=4)]: Done 117 tasks      | elapsed: 37.1min\n",
      "[Parallel(n_jobs=4)]: Done 118 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=4)]: Done 119 tasks      | elapsed: 37.6min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 37.9min\n",
      "[Parallel(n_jobs=4)]: Done 121 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=4)]: Done 122 tasks      | elapsed: 38.5min\n",
      "[Parallel(n_jobs=4)]: Done 123 tasks      | elapsed: 38.7min\n",
      "[Parallel(n_jobs=4)]: Done 124 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=4)]: Done 125 tasks      | elapsed: 39.2min\n",
      "[Parallel(n_jobs=4)]: Done 126 tasks      | elapsed: 43.3min\n",
      "[Parallel(n_jobs=4)]: Done 127 tasks      | elapsed: 43.5min\n",
      "[Parallel(n_jobs=4)]: Done 128 tasks      | elapsed: 43.8min\n",
      "[Parallel(n_jobs=4)]: Done 129 tasks      | elapsed: 43.9min\n",
      "[Parallel(n_jobs=4)]: Done 130 tasks      | elapsed: 44.8min\n",
      "[Parallel(n_jobs=4)]: Done 131 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=4)]: Done 132 tasks      | elapsed: 45.3min\n",
      "[Parallel(n_jobs=4)]: Done 133 tasks      | elapsed: 45.5min\n",
      "[Parallel(n_jobs=4)]: Done 134 tasks      | elapsed: 45.8min\n",
      "[Parallel(n_jobs=4)]: Done 135 tasks      | elapsed: 46.0min\n",
      "[Parallel(n_jobs=4)]: Done 136 tasks      | elapsed: 46.2min\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=4)]: Done 138 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=4)]: Done 139 tasks      | elapsed: 46.7min\n",
      "[Parallel(n_jobs=4)]: Done 140 tasks      | elapsed: 48.3min\n",
      "[Parallel(n_jobs=4)]: Done 141 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=4)]: Done 142 tasks      | elapsed: 50.8min\n",
      "[Parallel(n_jobs=4)]: Done 143 tasks      | elapsed: 51.2min\n",
      "[Parallel(n_jobs=4)]: Done 144 tasks      | elapsed: 52.3min\n",
      "[Parallel(n_jobs=4)]: Done 145 tasks      | elapsed: 52.5min\n",
      "[Parallel(n_jobs=4)]: Done 146 tasks      | elapsed: 53.0min\n",
      "[Parallel(n_jobs=4)]: Done 147 tasks      | elapsed: 53.4min\n",
      "[Parallel(n_jobs=4)]: Done 148 tasks      | elapsed: 53.6min\n",
      "[Parallel(n_jobs=4)]: Done 149 tasks      | elapsed: 53.7min\n",
      "[Parallel(n_jobs=4)]: Done 150 tasks      | elapsed: 53.7min\n",
      "[Parallel(n_jobs=4)]: Done 151 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=4)]: Done 152 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=4)]: Done 153 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 54.0min\n",
      "[Parallel(n_jobs=4)]: Done 155 tasks      | elapsed: 55.2min\n",
      "[Parallel(n_jobs=4)]: Done 156 tasks      | elapsed: 58.3min\n",
      "[Parallel(n_jobs=4)]: Done 157 tasks      | elapsed: 58.4min\n",
      "[Parallel(n_jobs=4)]: Done 158 tasks      | elapsed: 58.4min\n",
      "[Parallel(n_jobs=4)]: Done 159 tasks      | elapsed: 59.7min\n",
      "[Parallel(n_jobs=4)]: Done 160 tasks      | elapsed: 62.6min\n",
      "[Parallel(n_jobs=4)]: Done 161 tasks      | elapsed: 62.7min\n",
      "[Parallel(n_jobs=4)]: Done 162 tasks      | elapsed: 62.8min\n",
      "[Parallel(n_jobs=4)]: Done 163 tasks      | elapsed: 63.0min\n",
      "[Parallel(n_jobs=4)]: Done 170 out of 170 | elapsed: 66.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 170 out of 170 | elapsed: 66.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 1h06m19s\n",
      "\n",
      "============best params found {'batch_size': 500, 'solver': 'adam', 'activation': 'relu', 'alpha': 0.5}\n",
      "\n",
      "============Classification report\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['learning_rate' 'power_t'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8ba32e395fc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m                                           \u001b[1;34m'param_batch_size'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                           \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                           \u001b[1;34m'power_t'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                                          ]]\n\u001b[1;32m     54\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'rank test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'score test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'score train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'alpha'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'activation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'solver'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'learn rate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'invscaling power'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2033\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%s not in index'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobjarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['learning_rate' 'power_t'] not in index\""
     ]
    }
   ],
   "source": [
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Further customizing\n",
    "param_grid = [{'alpha' : np.logspace(-1, 1, 3),\n",
    "               'activation' : ['relu'],\n",
    "               'solver' : ['adam'],\n",
    "               'batch_size' : [200,  400, 600, 800]\n",
    "              },\n",
    "              {'alpha' : np.logspace(1, 4, 3),\n",
    "               'activation' : ['tanh'],\n",
    "               'solver' : ['lbfgs']\n",
    "              },\n",
    "             {'alpha' : [0.3, 0.5],\n",
    "               'activation' : ['relu'],\n",
    "               'solver' : ['adam'],\n",
    "               'batch_size' : [400, 500],\n",
    "               \n",
    "              },\n",
    "              {'alpha' : np.logspace(-1, 1, 3),\n",
    "               'activation' : ['relu'],\n",
    "               'solver' : ['sgd'],\n",
    "               'batch_size' : [200],\n",
    "               'learning_rate' : ['constant', 'adaptive']\n",
    "              }, \n",
    "              {'alpha' : np.logspace(-1, 1, 3),\n",
    "               'activation' : ['relu'],\n",
    "               'solver' : ['sgd'],\n",
    "               'batch_size' : [200],\n",
    "               'learning_rate' : ['invscaling'],\n",
    "               'power_t' : [0.2, 0.5, 0.7]\n",
    "              }]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(), param_grid=param_grid, cv=5, n_jobs=4, verbose=12)\n",
    "\n",
    "#Fitting the grid\n",
    "start = time.time()\n",
    "grid.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for GridSearch fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_alpha', \n",
    "                                          'param_activation',\n",
    "                                          'param_solver',\n",
    "                                          'param_batch_size',\n",
    "                                          'param_learning_rate',\n",
    "                                          'param_power_t'\n",
    "                                         ]]\n",
    "results.columns = ['rank test', 'score test', 'score train', 'alpha', 'activation', 'solver', 'batch', 'learn rate', 'invscaling power']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=grid.best_estimator_, trial_number=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============best params found {'batch_size': 500, 'solver': 'adam', 'activation': 'relu', 'alpha': 0.5}\n",
      "\n",
      "============Classification report\n",
      "    rank test  score test  score train    alpha activation solver batch  \\\n",
      "18          1    0.982131     0.994013      0.5       relu   adam   500   \n",
      "16          2    0.982083     0.997050      0.3       relu   adam   500   \n",
      "15          2    0.982083     0.995975      0.3       relu   adam   400   \n",
      "7           4    0.981761     0.992474        1       relu   adam   800   \n",
      "17          5    0.981667     0.992287      0.5       relu   adam   400   \n",
      "0           6    0.981610     0.997474      0.1       relu   adam   200   \n",
      "6           7    0.981420     0.989957        1       relu   adam   600   \n",
      "2           8    0.981042     0.999837      0.1       relu   adam   600   \n",
      "3           9    0.980966     0.999891      0.1       relu   adam   800   \n",
      "1          10    0.980871     0.999458      0.1       relu   adam   400   \n",
      "19         11    0.980331     0.992860      0.1       relu    sgd   200   \n",
      "20         12    0.980208     0.993104      0.1       relu    sgd   200   \n",
      "5          13    0.979943     0.986245        1       relu   adam   400   \n",
      "12         14    0.979110     0.999972       10       tanh  lbfgs   NaN   \n",
      "22         15    0.976818     0.980355        1       relu    sgd   200   \n",
      "21         16    0.976345     0.980327        1       relu    sgd   200   \n",
      "13         17    0.975767     0.978568  316.228       tanh  lbfgs   NaN   \n",
      "4          18    0.975568     0.978989        1       relu   adam   200   \n",
      "11         19    0.966070     0.967524       10       relu   adam   800   \n",
      "10         20    0.960710     0.961795       10       relu   adam   600   \n",
      "28         21    0.954555     0.956593        1       relu    sgd   200   \n",
      "25         22    0.953475     0.955594      0.1       relu    sgd   200   \n",
      "9          23    0.950303     0.950526       10       relu   adam   400   \n",
      "31         24    0.942311     0.942888       10       relu    sgd   200   \n",
      "23         25    0.937481     0.937817       10       relu    sgd   200   \n",
      "24         26    0.937358     0.937805       10       relu    sgd   200   \n",
      "14         27    0.935597     0.936042    10000       tanh  lbfgs   NaN   \n",
      "8          28    0.934697     0.934877       10       relu   adam   200   \n",
      "29         29    0.871496     0.871700        1       relu    sgd   200   \n",
      "33         30    0.868210     0.866785       10       relu    sgd   200   \n",
      "32         31    0.865218     0.866953       10       relu    sgd   200   \n",
      "26         32    0.859934     0.860672      0.1       relu    sgd   200   \n",
      "27         33    0.858712     0.859545      0.1       relu    sgd   200   \n",
      "30         34    0.854934     0.854593        1       relu    sgd   200   \n",
      "\n",
      "    learn rate invscaling power  \n",
      "18         NaN              NaN  \n",
      "16         NaN              NaN  \n",
      "15         NaN              NaN  \n",
      "7          NaN              NaN  \n",
      "17         NaN              NaN  \n",
      "0          NaN              NaN  \n",
      "6          NaN              NaN  \n",
      "2          NaN              NaN  \n",
      "3          NaN              NaN  \n",
      "1          NaN              NaN  \n",
      "19    constant              NaN  \n",
      "20    adaptive              NaN  \n",
      "5          NaN              NaN  \n",
      "12         NaN              NaN  \n",
      "22    adaptive              NaN  \n",
      "21    constant              NaN  \n",
      "13         NaN              NaN  \n",
      "4          NaN              NaN  \n",
      "11         NaN              NaN  \n",
      "10         NaN              NaN  \n",
      "28  invscaling              0.2  \n",
      "25  invscaling              0.2  \n",
      "9          NaN              NaN  \n",
      "31  invscaling              0.2  \n",
      "23    constant              NaN  \n",
      "24    adaptive              NaN  \n",
      "14         NaN              NaN  \n",
      "8          NaN              NaN  \n",
      "29  invscaling              0.5  \n",
      "33  invscaling              0.7  \n",
      "32  invscaling              0.5  \n",
      "26  invscaling              0.5  \n",
      "27  invscaling              0.7  \n",
      "30  invscaling              0.7  \n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0h00m00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:15: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_alpha', \n",
    "                                          'param_activation',\n",
    "                                          'param_solver',\n",
    "                                          'param_batch_size',\n",
    "                                          'param_learning_rate',\n",
    "                                          'param_power_t'\n",
    "                                         ]]\n",
    "results.columns = ['rank test', 'score test', 'score train', 'alpha', 'activation', 'solver', 'batch', 'learn rate', 'invscaling power']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=grid.best_estimator_, trial_number=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for GridSearch fitting: 0h05m19s\n",
      "\n",
      "============best params found {'batch_size': 500, 'activation': 'relu', 'hidden_layer_sizes': (250,), 'solver': 'adam', 'alpha': 0.3}\n",
      "\n",
      "============Classification report\n",
      "   rank test  score test  score train alpha activation solver batch  layers\n",
      "4          1    0.982491     0.997659   0.3       relu   adam   500  (250,)\n",
      "3          2    0.982273     0.997441   0.3       relu   adam   500  (126,)\n",
      "0          3    0.982225     0.996773   0.3       relu   adam   450     NaN\n",
      "2          4    0.982036     0.997334   0.3       relu   adam   500  (100,)\n",
      "1          5    0.981638     0.997088   0.3       relu   adam   500     NaN\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n",
      "total timed used for predicting: 0h00m00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:37: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Further customizing\n",
    "param_grid = [{'alpha' : [0.3],\n",
    "               'activation' : ['relu'],\n",
    "               'solver' : ['adam'],\n",
    "               'batch_size' : [450,500]\n",
    "              },\n",
    "             {'alpha' : [0.3],\n",
    "               'activation' : ['relu'],\n",
    "               'solver' : ['adam'],\n",
    "               'batch_size' : [500],\n",
    "               'hidden_layer_sizes': [(100,), (126,), (250,)]\n",
    "              }]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(MLPClassifier(), param_grid=param_grid, cv=5, n_jobs=4, verbose=5)\n",
    "\n",
    "#Fitting the grid\n",
    "start = time.time()\n",
    "grid.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for GridSearch fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Feedback on the best parameters, and each parameter performance:\n",
    "print(\"\\n============best params found \\n %a\"%(grid.best_params_))\n",
    "print('\\n============Classification report')\n",
    "results = pd.DataFrame(grid.cv_results_)[['rank_test_score',\n",
    "                                          'mean_test_score', \n",
    "                                          'mean_train_score', \n",
    "                                          'param_alpha', \n",
    "                                          'param_activation',\n",
    "                                          'param_solver',\n",
    "                                          'param_batch_size',\n",
    "                                          'param_hidden_layer_sizes'\n",
    "                                         ]]\n",
    "results.columns = ['rank test', 'score test', 'score train', 'alpha', 'activation', 'solver', 'batch', 'layers']\n",
    "results = results.sort(columns='rank test', ascending=True)\n",
    "print(results)\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=grid.best_estimator_, trial_number=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble methods + Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  6.1min remaining:  6.1min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  6.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  6.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0h06m34s\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   19.7s remaining:   19.7s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   20.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   20.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m20s\n",
      "Score with basic personnalized estimator 0.054\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[50089  2697    14]\n",
      " [    0     0     0]\n",
      " [   11  2745 50044]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.4s remaining:    2.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "#Parameters as defined by the previous study... \n",
    "#We might however want to reduce the depth\n",
    "#change the number of estimators\n",
    "\n",
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228)\n",
    "clf = BaggingClassifier(base_estimator=clf_bag, n_estimators=70, n_jobs=4, verbose=5, max_samples=0.3, oob_score=True)\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.85)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with bagging + MPLClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=17, threshold=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator so far (score on the testing set) is \n",
    "```python\n",
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228) #best so far\n",
    "clf = BaggingClassifier(base_estimator=clf_bag, n_estimators=12, n_jobs=4, verbose=5)\n",
    "```\n",
    "\n",
    "Oh no, wait ! Even better is :\n",
    "\n",
    "```python\n",
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228) #best so far\n",
    "clf = BaggingClassifier(base_estimator=clf_bag, n_estimators=20, n_jobs=4, verbose=5, max_samples=0.5, oob_score=True)\n",
    "```\n",
    "\n",
    "As a matter of facts it is with a threshold of 0.85:\n",
    "\n",
    "```python\n",
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228)\n",
    "clf = BaggingClassifier(base_estimator=clf_bag, n_estimators=40, n_jobs=4, verbose=5, max_samples=0.5, oob_score=True)\n",
    "```\n",
    "\n",
    "Best score is currently in the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons + Bagging + Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  3.5min remaining:  3.5min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  3.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  4.9min remaining:  4.9min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  5.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  5.2min finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  5.9min remaining:  5.9min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  6.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  6.2min finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  7.2min remaining:  7.2min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  7.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  7.4min finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  8.9min remaining:  8.9min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  9.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 1h13m05s\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   11.6s remaining:   11.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.1s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   10.7s remaining:   10.7s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   11.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   11.3s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   12.3s remaining:   12.3s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   11.1s remaining:   11.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   11.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   11.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   11.4s remaining:   11.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   11.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   11.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h01m01s\n",
      "Score with bagging + MPLClassifier estimator 0.043\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[50695  2090    15]\n",
      " [    0     0     0]\n",
      " [   18  2071 50711]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228)\n",
    "clf1 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.2, oob_score=True)\n",
    "clf2 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.4, oob_score=True)\n",
    "clf3 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.6, oob_score=True)\n",
    "clf4 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.8, oob_score=True)\n",
    "clf5 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=1.0, oob_score=True)\n",
    "clf = VotingClassifier(estimators=[('clf1', clf1),('clf2', clf2),('clf3', clf3),('clf4', clf4),('clf5', clf5)], n_jobs=4, voting='soft')\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf1.fit(X_train_adapt, y_train_adapt)\n",
    "clf2.fit(X_train_adapt, y_train_adapt)\n",
    "clf3.fit(X_train_adapt, y_train_adapt)\n",
    "clf4.fit(X_train_adapt, y_train_adapt)\n",
    "clf5.fit(X_train_adapt, y_train_adapt)\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.82)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with bagging + MPLClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=18, threshold=0.82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OMG\n",
    "\n",
    "![Yeah!](https://s-media-cache-ak0.pinimg.com/236x/5d/77/05/5d7705ea5bbb9f0f93b239971ad02078.webp)\n",
    "---\n",
    "> Now I got the #2nd best classifier with\n",
    "```python\n",
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228)\n",
    "clf1 = BaggingClassifier(base_estimator=clf_bag, n_estimators=40, n_jobs=4, verbose=5, max_samples=0.3, oob_score=True)\n",
    "clf2 = BaggingClassifier(base_estimator=clf_bag, n_estimators=40, n_jobs=4, verbose=5, max_samples=0.2, oob_score=True)\n",
    "clf3 = BaggingClassifier(base_estimator=clf_bag, n_estimators=40, n_jobs=4, verbose=5, max_samples=0.7, oob_score=True)\n",
    "clf4 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=1.0, oob_score=True)\n",
    "clf = VotingClassifier(estimators=[('clf1', clf1),('clf2', clf2),('clf3', clf3),('clf4', clf4)], n_jobs=4, voting='soft')\n",
    "```\n",
    "and a probability threshold of 0.82 to decide if the 0 label should be invoked\n",
    "\n",
    "Below is my last attempt to perform better: it is the same thing, but with a different preprocessing (without PCA). It was not conclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 1h39m50s\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   13.0s remaining:   13.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.5s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   12.4s remaining:   12.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   13.1s remaining:   13.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.6s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   12.8s remaining:   12.8s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   12.5s remaining:   12.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   12.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h01m07s\n",
      "Score with bagging + MPLClassifier estimator 0.047\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[50621  2144    35]\n",
      " [    0     0     0]\n",
      " [   33  2093 50674]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.6s remaining:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.6s remaining:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s finished\n"
     ]
    }
   ],
   "source": [
    "#Scale it \n",
    "myScaler = StandardScaler()\n",
    "X_train_scaled = myScaler.fit_transform(X_train)\n",
    "\n",
    "#Shuffle it\n",
    "X_train_scaled_shuffled, y_train_scaled_shuffled = shuffle(X_train_scaled, y_train)\n",
    "\n",
    "#Adapt the test set accordingly\n",
    "X_test_scaled = myScaler.transform(X_test)\n",
    "\n",
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228)\n",
    "clf1 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.2, oob_score=True)\n",
    "clf2 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.3, oob_score=True)\n",
    "clf3 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.4, oob_score=True)\n",
    "clf4 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.5, oob_score=True)\n",
    "clf5 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=5, max_samples=0.6, oob_score=True)\n",
    "clf = VotingClassifier(estimators=[('clf1', clf1),('clf2', clf2),('clf3', clf3),('clf4', clf4),('clf5', clf5)], n_jobs=4, voting='soft')\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "# clf1.fit(X_train_adapt, y_train_adapt)\n",
    "# clf2.fit(X_train_adapt, y_train_adapt)\n",
    "# clf3.fit(X_train_adapt, y_train_adapt)\n",
    "# clf4.fit(X_train_adapt, y_train_adapt)\n",
    "# clf5.fit(X_train_adapt, y_train_adapt)\n",
    "clf.fit(X_train_scaled_shuffled, y_train_scaled_shuffled)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_scaled_shuffled, clf=clf, threshold=0.82)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_scaled_shuffled, y_pred_train)\n",
    "print(\"Score with bagging + MPLClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_scaled_shuffled, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_scaled, clf=clf, trial_number=19, threshold=0.82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bag = MLPClassifier(batch_size=400, activation='relu', solver='adam', alpha=0.316228)\n",
    "clf_bag2 = GradientBoostingClassifier(n_estimators= 100, learning_rate=0.1, max_depth=7)\n",
    "clf_bag3 = SVC(kernel='rbf', gamma='auto', C=100, cache_size=2500, probability=True)\n",
    "\n",
    "clf1 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=1, max_samples=0.5, oob_score=True)\n",
    "clf2 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=1, max_samples=1.0, oob_score=True)\n",
    "\n",
    "clf3 = BaggingClassifier(base_estimator=clf_bag2, n_estimators=5, n_jobs=4, verbose=1, max_samples=0.5, oob_score=True)\n",
    "clf4 = BaggingClassifier(base_estimator=clf_bag3, n_estimators=5, n_jobs=4, verbose=1, max_samples=0.5, max_features=75, oob_score=True)\n",
    "clf5 = BaggingClassifier(base_estimator=clf_bag, n_estimators=50, n_jobs=4, verbose=1, max_samples=0.2, oob_score=True)\n",
    "clf = VotingClassifier(estimators=[('clf1', clf1),('clf2', clf2),('clf3', clf3),('clf4', clf4),('clf5', clf5)], n_jobs=4, voting='soft')\n",
    "\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf1.fit(X_train_adapt, y_train_adapt)\n",
    "clf2.fit(X_train_adapt, y_train_adapt)\n",
    "clf3.fit(X_train_adapt, y_train_adapt)\n",
    "clf4.fit(X_train_adapt, y_train_adapt)\n",
    "clf5.fit(X_train_adapt, y_train_adapt)\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.82)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with bagging + MPLClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=18, threshold=0.82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ZUT!](http://www.femoticons.net/images/posts/crying_emoticon_for_facebook.jpg)\n",
    "I am now only #23 out of the 77 contestants. I must try to find an even better solution\n",
    "\n",
    "## Desperate trial that shows it is still possible to make score better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Come Back on simpler methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  2.0min remaining:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0h02m01s\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    7.6s remaining:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m08s\n",
      "Score with bagging + MPLClassifier estimator 0.087\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[48794  3947    59]\n",
      " [    0     0     0]\n",
      " [   90  3786 48924]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "clf_bag = MLPClassifier(batch_size='auto', activation='relu', solver='adam', alpha=0.1, tol=0.001)\n",
    "clf = BaggingClassifier(base_estimator=clf_bag, \n",
    "                        n_estimators=40, \n",
    "                        max_samples=0.3,\n",
    "                        max_features= 0.6,\n",
    "                        n_jobs=4, \n",
    "                        verbose=5)\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_scaled_shuffled, y_train_scaled_shuffled)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_scaled_shuffled, clf=clf, threshold=0.73)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_scaled_shuffled, y_pred_train)\n",
    "print(\"Score with bagging + MPLClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_scaled_shuffled, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_scaled, clf=clf, trial_number=20, threshold=0.73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is slightly better than before on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 features selected out of 128 (98 %) for PCA which explains 99 % of variance\n",
      "105600 observations selected out of 105600 (100 %) for Shuffling and training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:211: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  indices = indices[:max_n_samples]\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:  2.7min remaining:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time used for fitting: 0h02m46s\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    8.1s remaining:    8.1s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    8.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m09s\n",
      "Score with bagging + MPLClassifier estimator 0.068\n",
      "\n",
      "\n",
      "Confusion matrix\n",
      "[[50055  2650    95]\n",
      " [    0     0     0]\n",
      " [  134  2211 50455]]\n",
      "\n",
      "Now for the test set\n",
      "0_labels enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    1.5s remaining:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total timed used for predicting: 0h00m02s\n"
     ]
    }
   ],
   "source": [
    "#Customizing the number of features and observations\n",
    "X_train_adapt, y_train_adapt, X_test_adapt = prepare_dataset(X_train, y_train, X_test, var_ratio_min=99.9, ratio_sd=100)\n",
    "\n",
    "clf_bag = MLPClassifier(batch_size='auto', activation='relu', solver='adam', alpha=0.1, tol=0.0007)\n",
    "clf = BaggingClassifier(base_estimator=clf_bag, \n",
    "                        n_estimators=40, \n",
    "                        max_samples=0.35,\n",
    "                        max_features= 0.62,\n",
    "                        n_jobs=4, \n",
    "                        verbose=5)\n",
    "#Fitting\n",
    "start = time.time()\n",
    "clf.fit(X_train_adapt, y_train_adapt)\n",
    "print(\"total time used for fitting: %s\"%(makeTimeSignificant(time.time() - start)))\n",
    "\n",
    "#Predicting\n",
    "y_pred_train = predict_0_labels(XX=X_train_adapt, clf=clf, threshold=0.73)\n",
    "\n",
    "#Score\n",
    "score = compute_pred_score(y_train_adapt, y_pred_train)\n",
    "print(\"Score with bagging + MPLClassifier estimator %0.3f\"%(score))\n",
    "print(\"\\n\\nConfusion matrix\")\n",
    "print(confusion_matrix(y_train_adapt, y_pred_train))\n",
    "\n",
    "#Saving results\n",
    "print(\"\\nNow for the test set\")\n",
    "save_prediction(X_test=X_test_adapt, clf=clf, trial_number=21, threshold=0.73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That was Even better !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
